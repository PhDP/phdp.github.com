<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Markov Logic: Logic, Probability, Learning</title>

    <meta name="description" content="Markov Logic: Logic, Probability, Learning">
    <meta name="author" content="Philippe Desjardins-Proulx">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/evoml2.css" id="theme">
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <script>
      if (window.location.search.match(/print-pdf/gi)) {
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'css/print/pdf.css';
        document.getElementsByTagName('head')[0].appendChild(link);
      }
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">
      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Markov Logic</h1>
          <h2>Logic, Probability, Learning</h2>
          <p>
            <small>by <a href="http://phdp.github.io/">Philippe Desjardins-Proulx</a></small>
          </p>
        </section>


        <section>
          <section>
            <h1>Motivation</h1>
          </section>
          <section>
            <table>
              <tr>
                <th>Language</th>
                <th>Ontological commitment</th>
                <th>Epistemological commitment</th>
              </tr>
              <tr>
                <td>First-Order Logic</td>
                <td>Facts, objects, relations</td>
                <td>True | False | Unknown</td>
              </tr>
              <tr>
                <td>Probability Theory</td>
                <td>Facts</td>
                <td>Degree of belief \(\in [0, 1]\)</td>
              </tr>
              <tr>
                <td>Markov Logic</td>
                <td>Facts, objects, relations</td>
                <td>Degree of belief \(\in [0, 1]\)</td>
              </tr>
            </table>
            <p><i>Adapted from <a href='http://aima.cs.berkeley.edu/'>Russell &amp; Norvig</a> (3rd ed., p 290).</i></p>
          </section>

          <section>
            <h2>In a nutshell</h2>
            <ol style='list-style-type: upper-roman'>
              <li>...Markov logic is about learning first-order logic sentences, each assigned with a probability \(\in [0, 1]\).</li>
              <li>Probability theory offers soft constraints, a world where many sentences are proven wrong is not impossible, but less
              likely.</li>
              <li>The more confident we are in a sentence contradicted by some evidence, the less likely the world is.</li>
              <li>Markov logic is a stict superset of first-order logic since \(p = 1.0\) is equivalent to <b>true</b>, and \(p = 0.0\) is equivalent to <b>false</b>.</li>
              <li>And it also a subperset of discrete and finite-precision probability distributions. That is: it has been proven that every probability distribution over discrete or finite-precision numeric variables can be
represented as a Markov logic network.</li>
            </ol>
          </section>

          <section>
            <h2>The case for Markov Logic</h2>
            <ol style='list-style-type: upper-roman'>
              <li>Logic for complexity. Probability theory for uncertainty (<a href='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.9497&rep=rep1&type=pdf'>e.g. collective classification of links</a>).</li>
              <li>Not a black box: FOL sentences are easy to understand (<a href='http://www.biomedcentral.com/1471-2105/14/273'>example</a>).</li>
              <li>Facilitates human interventions (knowledge engineering).</li>
              <li>Facilitates computer interventions (e.g. Mihalkova's transfer algorithm).</li>
            </ol>
          </section>
        </section>

        <section>
          <section>
            <h1>First-Order Logic</h1>
          </section>
          <section>
            <h2>How expressive / powerful is first-order logic?</h2>
            <p>The general consensus is that it's powerful enough to formalize much of modern mathematics, but not enough for some nuances of natural languages.</p>
            <p>Bottom line: it's <b>very</b> expressive, and since it's enough for modern mathematics, it's probably enough for most (or all) knowledge engineering.</p>
          </section>
          <section>
            <h2>Terms</h2>
            <p><i>...refer to objects, which can be anything from integers to species, cities, words...</i></p>
            <ol style='list-style-type: upper-roman'>
              <li><b>Variables</b>, e.g. \(x, y, z\). The convention is to start variable names with a lowercase character. A variable ranges over objects of a certain type.</p>
              <li><b>Constants</b>, e.g \(Wolf, 0, Tokyo\), represent actual objects in the domain.</li>
              <li><b>Functions</b>, which are mappings between 0 o more terms and another term.</li>
            </ol>
          </section>
          <section>
            <h2>Atomic sentences</h2>
            <p><i>An atomic sentence is an element that, alone, is a valid first-order logic sentence.</i></p>
            <ol style='list-style-type: upper-roman'>
              <li><b>True</b> or <i>top</i>: \(\top\), and it's negation <b>False</b> or <i>bottom</i>: \(\bot\).</li>
              <li><b>Predicates</b>, which are mappings between 0 or more <b>terms</b> to a truth value. They have the same form as functions but can be distinguished by the context: predicates are atomic sentences, not functions.</li>
              <li><b>Identity</b>, represented by the mysterious = symbol, tests if two terms are the same. Identity can be (and often is) seen as a predicate with two arguments.</li>
            </ol>
          </section>
          <section>
            <h2>A few sentences</h2>
            <p style='color:#10CC10'>\[PrimeNumber(11)\]</p>
            <p style='color:#10CC10'>\[PrimeNumber(Minus(8, 1))\]</p>
            <p style='color:#10CC10'>\[\top\]</p>
            <p style='color:#10CC10'>\[FatherOf(Jesus) = God\]</p>
            <p style='color:#10CC10'>\[Identity(FatherOf(Jesus), God)\]</p>
            <p style='color:#C24617'>\[Dist(Helsinki, Jerusalem)\]</p>
            <p style='color:#10CC10'>\[Dist(Helsinki, Jerusalem) = 7653.8km\]</p>
            <p style='color:#10CC10'>\[GreaterThan(Dist(Helsinki, Jerusalem), Dist(Montreal, Quebec))\]</p>
            <p style='color:#10CC10'>\[Dist(Helsinki, Jerusalem) > Dist(Montreal, Quebec)\]</p>
          </section>
          <section>
            <h2>Connectives</h2>
            <p><i>...connect sentences</i></p>
            <ol style='list-style-type: upper-roman'>
              <li>The binary connective <b>and</b>: \(x \land y\), which is true only if both \(x\) and \(y\) are true. Like all other connective shown here, if \(x\) and \(y\) are sentences, then \(x \land y\) is also a valid sentence.</li>
              <li>The binary connective <b>or</b>: \(x \lor y\), which is true only if \(x\) is true, if \(y\) is true, or if both are true.</li>
              <li>The binary connective <b>implies</b>: \(x \implies y\), returns true in all cases, except if \(x\) is true and \(y\) is false.</li>
              <li>The binary connective <b>iff</b>: \(x \leftrightarrow y\), returns true if \(x\) and \(y\) have the same value, that is if they are both true, or both false.</li>
              <li>The binary connective <b>xor</b> (exclusive or): \(x \oplus y\), returns true if \(x\) and \(y\) have different values.</li>
              <li>The unary connective <b>not</b>: \(\lnot x\), which is true only if \(x\) is false.</li>
              <li>The qualifiers <b>for all</b> and <b>exists</b> (respectively \(\forall\) and \(\exists\)).</li>
            </ol>
          </section>
          <section>
            <h2>Grounding</h2>
            <p><i>We talk of a ground term or ground sentence if no variables are present.</i></p>
            <p style='color:#C24617'>\[\forall y \exists x, y \neq White \implies BrighterThan(x, y)\]</p>
            <p style='color:#10CC10'>\[BrighterThan(Pink, Red)\]</p>
          </section>
          <section>
            <h2>Second and Higher-Order Logic</h2>
            <p style='color:#10CC10'>\[\forall n\ GermainPrime(n) \implies PrimeNumber(2p + 1)\]</p>
            <p style='color:#10CC10'>\[\forall n\ GermainPrime(n) \implies PrimeNumber(Add(Multiply(2, p), 1))\]</p>
            <p style='color:#10CC10'>\[\forall x\ Pierre(x) \land Roule(x) \implies \lnot AmasseMousse(x)\]</p>
            <p>Santa Claus is a sadist</p>
            <p style='color:#10CC10'>\[Sadist(SantaClaus)\]</p>
            <p>Santa Claus has all the attributes of a sadist</p>
            <p style='color:#C24617'>\[\forall foo(Sadist)\ foo(SantaClaus)\]</p>
          </section>
          <section>
            <h2>Example 0: A bit of arithmetics</h2>
            <ol style='list-style-type: upper-roman'>
              <li>Let's define the predicates GreaterThan, SmallerThan, Equal, each taking two real numbers.</li>
              <li>Then, we can define the functions Addition, Multiplication, each taking two arguments (real numbers) and returning a real number.</li>
            </ol>
            <p style='color:#10CC10'>\[42 > 2 \times 6\]</p>
            <p style='color:#10CC10'>\[GreaterThan(Addition(40, 2), Multiply(2, 6))\]</p>
            <p style='color:#10CC10'>\[\forall x\ GreaterThan(1, Multiply(0, x))\]</p>
            <p style='color:#10CC10'>\[\forall x, y\ Equals(Addition(x, y), Addition(y, x))\]</p>
            <p style='color:#10CC10'>\[\lnot \exists x, y, z\ Equals(z, Addition(x, y)) \land GreaterThan(z, Addition(x, y)) \]</p>
            <p style='color:#10CC10'>\[GreaterThan(2, 6)\]</p>
            <p style='color:#C24617'>\[Multiply(2, 6)\]</p>
          </section>
          <section>
            <h2>Logic as a Metalanguage</h2>
            <p>A <i>language</i> is the set of objects, functions, and predicates allowed, along with methods to evaluate them.</p>
            <p>E.g. we can evaluate a sentence with \(Presence(GreyWolf, France)\) by providing the answer to an evaluation function, or have it look at a database.</p>
          </section>
          <section>
            <h2>Example 1: Peano axioms</h2>
            <p>\[NatNum(0)\]</p>
            <p>\[\forall n\ NatNum(n) \implies NatNum(S(n))\]</p>
            <p>\[\forall n\ 0 \not= S(n)\]</p>
            <p>\[\forall m, n\ m \not = n \implies S(m) \not = S(n)\]</p>
            <p>\[\forall m\ NatNum(m) \implies Addition(0, m) = m\]</p>
            <p>\[\forall m, n\ NatNum(m) \land NatNum(n) ⇒ Addition(S(m), n) = S(Addition(m, n))\]</p>
          </section>
          <section>
            <h2>Knowledge engineering</h2>
            <p>Knowledge engineering is the task of establishing the rules for some system. It is common in engineering, business
            intelligence, health informatics, chemo-informatics, logisic problems, etc etc. Normally, it is done by deciding on a
            set of first-order sentences.</p>
          </section>
          <section>
            <h2>Example 2: Ecology?</h2>
            <p>\[s0: Presence(GreyWolf)\]</p>
            <p>\[s1: \forall x,y,z\ PreyOn(x, z) \land PreyOn(y, z) \implies Compete(x, y)\]</p>
            <p>\[s2: \forall x,y\ PreyOn(x, y) \implies Larger(x, y) \lor Parasite(x)\]</p>
            <p>\[s3: \forall x,y\ SameNiche(x, y) \implies \lnot CoOccur(x, y)\]</p>
            <p>\[s4: \exists x\ NumPreys(x) = 0\]</p>
            <p>\[s5: \forall x,y,z\ NumPreys(x) = 1 \land NumPreys(y) = 1 \land PreyOn(x, z) \land\]</p>
            <p>\[PreyOn(y, z) \implies \lnot CoOccur(x, y)\]</p>
          </section>
          <section>
            <h2>A Few Gotchas</h2>
            <p>Implication works well with the \(\forall\) qualifier:</p>
            <p>\[\forall x,y,z\ PreyOn(x, z) \land PreyOn(y, z) \implies Compete(x, y)\]</p>
            <p>\[\forall x,y,z\ PreyOn(x, z) \land PreyOn(y, z) \land Compete(x, y)\]</p>
            <p>...but be careful when the left side is rarely true (solution: types), it messes up probabilistic inference. Think of the above sentences if x, y, z ranges over all objects (cities, people, species) vs only species.</p>
          </section>
          <section>
            <h2>Qualification order</h2>
            <p>"Everybody loves somebody":</p>
            <p>\[\forall x \exists y\ Loves(x, y)\]</p>
            <p>"There is someone who is loved by everyone":</p>
            <p>\[\exists y \forall x\ Loves(x, y)\]</p>
            <p>The qualfications should be read:</p>
            <p>\[\forall x\ (\exists y\ Loves(x, y))\]</p>
          </section>
          <section>
            <h2>Order of Precedence</h2>
            <p>...differs from author to author.</p>
            <p>A common scheme: \[\lnot, =, \land, \lor, \implies, \oplus, \iff\].</p>
          </section>
          <section>
            <p>My FOL parser (<a href="https://github.com/PhDP/Gryffon">https://github.com/PhDP/Gryffon</a>)</p>

<div class='terminal'><pre>$ git clone https://github.com/PhDP/Gryffon.git</pre></div>

<div class='terminal'><pre>$ cd Gryffon</pre></div>

<div class='terminal'><pre>$ sbt console</pre></div>

<pre><code class="scala">scala> parseSentence("Forall m, n m != n => S(m) != S(n)")
res0: Option[gryffon.reasoning.Sentence] = Some(∀m ∀n m != n ⇒ S(m) != S(n))

scala> res0.get.showStruct
res1: String = ForAll(Variable(m), ForAll(Variable(n), Implies(Not(Identity(Variable(m), Variable(n))), Not(Identity(Function(S, List(Variable(m)), Function(S, List(Variable(n)))))))

scala> parseSentence("Equals(Addition(1, 2), 3)")
res2: Option[gryffon.reasoning.Sentence] = Some(Equals(Addition(1, 2), 3))

scala> res2.get.showStruct
res3: String = Predicate(Equals, List(Function(Addition, List(Constant(1), Constant(2)), Constant(3))</code></pre>

          </section>

          <section>
<div class='terminal'><pre>$ cat data/Ecology.txt
Forall x,y,z  PreyOn(x, z) and PreyOn(y, z) => Compete(x, y)
Forall x,y    PreyOn(x, y) => Larger(x, y) or Parasite(x)
Forall x,y    SameNiche(x, y) => !CoOccur(x, y)
Exists x      NumPreys(x) = 0
$ sbt console</pre></div>

<pre><code class="scala">scala> import scala.io.Source
scala> val lines = Source.fromFile("data/Ecology.txt").getLines
scala> lines.foreach { s => println(parseSentence(s).get) }
∀x ∀y ∀z PreyOn(x, z) ∧ PreyOn(y, z) ⇒ Compete(x, y)
∀x ∀y PreyOn(x, y) ⇒ Larger(x, y) ∨ Parasite(x)
∀x ∀y SameNiche(x, y) ⇒ ¬CoOccur(x, y)
∃x NumPreys(x) = 0
</code></pre>
          </section>

        </section>

        <section>
          <section>
            <h1>Markov Networks</h1>
          </section>
          <section>
            <p>Also known as <b>undirected graphical models</b> or <b>Markov random fields</b> (from their origin in statistical physics).</p>
            <p>A Markov Network is an undirected graph used to compress complex probability distributions into a manageable structure.</p>
            <p>They are mostly defined by maximal cliques and the resulting bipartite factor graph.</p>
            <p>Most (deep) neural network models (e.g. Boltzman machines) are either Markov networks, or closely related.</p>
          </section>
          <section>
            <p><img src='img/markov_net.svg' alt='Markov network'/></p>
            <p>A Markov Network with 8 variables, 9 edges, and 3 factors:</p>
            \[MaxCliques: [(a, b, c), (d), (e, f, g)]\]
            <p>The factor graph in red has square vertices for factors (or features), linked to the vertices holding the variables.</p>
          </section>
          <section>
            <p><img src='img/markov_net.svg' alt='Markov network'/></p>
            \[P(a, b, c, d, e, f, g) = \frac{1}{Z}\phi_0(a, b, c)\phi_1(d)\phi_2(e, f, g, h)\]
          </section>
          <section>
            <p>Markov network are often used as log-linear models:</p>
            <p>\[P(X = x) = \frac{1}{Z}\exp\left(\sum_j w_j f_j(x)\right).\]
            <p>With \(Z\) being a factor for normalization.</p>
            <p>There is no closed form solution for maximum likelihood or maximum a posteriori probability, but since
            the function is concave, it is fairly easy to compute with gradient methods.</p>
          </section>
        </section>

        <section>
          <section>
            <h1>Markov Logic Networks</h1>
          </section>
          <section>
            <h2>The Gist of It</h2>
            <p>A Markov Logic Network \(L\) is a set of tuples made of first-order logic sentences and weights: \(L = (s_0, w_0), (s_1, w_1), (s_2, w_2), ...\)</p>
<div class='terminal'><pre>0.9 Forall x,y,z  PreyOn(x, z) and PreyOn(y, z) => Compete(x, y)
1.4 Forall x,y    PreyOn(x, y) => Larger(x, y) or Parasite(x)
1.1 Forall x,y    SameNiche(x, y) => !CoOccur(x, y)
0.8 Exists x      NumPreys(x) = 0
</pre></div>
          </section>
          <section>
            <h2>Where's my Network?</h2>
            <p>Together with a finite set of constants \(C = c_0, c_1, ...\), the Markov logic network \(L\) defines
            a Markov network \(M_{L,C}\) as follow:</p>
            <ol style='list-style-type: upper-roman'>
              <li>\(M_{L, C}\) contains one binary vertex for each possible grounding of each predicate appearing in \(L\).
The value of the vertex is 1 if the ground predicate is true, and 0 otherwise.</li>
              <li>\(M_{L, C}\) contains one feature (vertex of the factor graph) for each possible grounding of each sentence \(s_i \in L\). The value of
this feature is 1 if the ground formula is true, and 0 otherwise. The weight of the feature is the
weight associated with \(s_i\) in \(L\).</li>
            </ol>
          </section>
          <section>
            <h2>The facts</h2>
<div class='terminal'><pre>Friends(Anna, Bob)
Friends(Anna, Edward)
Friends(Anna, Frank)
Friends(Edward, Frank)
Friends(Gary, Helen)
!Friends(Gary, Frank)
Smokes(Anna)
Smokes(Edward)</pre></div>
          <p>Facts are just ground first-order logic sentences:</p>
<pre><code class="scala">scala> import scala.io.Source
scala> val lines = Source.fromFile("data/smoke/evidence.db").getLines
scala> lines.foreach { s => val x = parseSentence(s).get; println("%-24s %s".format(x, x.showStruct)) }
Friends(Anna, Bob)       Predicate(Friends, List(Constant(Anna), Constant(Bob))
Friends(Anna, Edward)    Predicate(Friends, List(Constant(Anna), Constant(Edward))
Friends(Anna, Frank)     Predicate(Friends, List(Constant(Anna), Constant(Frank))
Friends(Edward, Frank)   Predicate(Friends, List(Constant(Edward), Constant(Frank))
Friends(Gary, Helen)     Predicate(Friends, List(Constant(Gary), Constant(Helen))
¬Friends(Gary, Frank)    Not(Predicate(Friends, List(Constant(Gary), Constant(Frank)))
Smokes(Anna)             Predicate(Smokes, List(Constant(Anna))
Smokes(Edward)           Predicate(Smokes, List(Constant(Edward))</code></pre>
          </section>
          <section>
            \[s_0: \forall x\ Smoking(x) \implies Cancer(x)\]
            \[s_1: \forall x, y\ Friend(x, y) \land Smoking(x) \implies Smoking(y)\]
            \[L: (s_0, 1.5), (s_1, 1.1)\]
            \[C: Anna, Bob\]
            <p><img src='img/mln.png' alt='Markov logic network'/></p>
            <p>Lots of possible things to do, such as updating the probabilities with the formula, evaluating \(P(Cancer(x))\), ...</p>
          </section>
        </section>

        <section>
          <section>
            <h1>References</h1>
            <ul>
              <li>Domingos and Lowd's <i><a href='http://www.morganclaypool.com/doi/abs/10.2200/S00206ED1V01Y200907AIM007?journalCode=aim'>Markov Logic: An Interface Layer for Artificial Intelligence</a></i> (2009) for a detailed explanation of Markov logic.</li>
              <li>Koller and Friedman's <i><a href='http://mitpress.mit.edu/books/probabilistic-graphical-models'>Probabilistic Graphical Models</a></i> (2009) for a good introduction to Markov networks.</li>
              <li>Russel and Norvig's <i><a href='http://aima.cs.berkeley.edu/'>Artificial Intelligence</a></i> (2009) for a gentle but smart introduction to logic.</li>
              <li>Harrison's <i><a href='http://www.cl.cam.ac.uk/~jrh13/atp/'>Handbook Of Practical Logic And Automated Reasoning</a></i> (damn it, also published in 2009), for algorithms to handle logic.</li>
              <li><a href='http://openlogicproject.org/'>The Open Logic Text</a> for a free and updated textbook on logic.</li>
              <li>Nath and Domingos' new paper <a href='http://homes.cs.washington.edu/~pedrod/papers/aaai15.pdf'><i>Learning Relational Sum-Product Networks</i></a>, for a new approach to probabilistic logic similar to Markov logic, but relying on deep networks.</li>
            </ul>
          </section>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    <script>
      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none

        // Parallax scrolling
        // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
        // parallaxBackgroundSize: '2100px 900px',

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
      });
    </script>
    <script type="text/javascript" src="../js/bower/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </body>
</html>
